{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":"<p>This wiki is for those who also study or are interested in computer science or artificial intelligence. In my learning approach, I search scattered documentation and random blog posts, or ask LLMs help, which takes up countless hours on those things. This wiki is my attempt to record these tips, gotchas, and my experience to save your and my time.</p> <p>Full wiki:</p> <ul> <li>AI\\ML</li> <li>Hardware </li> <li>Robotics </li> <li>Systems </li> <li>Programming </li> <li>Tools</li> <li>Papers</li> </ul> <p>This wiki is keeping updated.</p> <p>Feel free to explore. Start wherever you need help.</p>"},{"location":"AI%5CML/index.html","title":"AI\\ML","text":""},{"location":"AI%5CML/index.html#frameworks","title":"Frameworks","text":"<ul> <li>Pytorch</li> <li>MXNet</li> <li>Hugging Face Transformers</li> </ul>"},{"location":"AI%5CML/index.html#libraries","title":"Libraries","text":"<ul> <li>Numpy</li> <li>Pandas</li> <li>Matplotlib</li> <li>Seaborn</li> <li>Scikit Learn</li> <li>Pillow</li> <li>Ultralytics</li> <li>Hugging Face Hub</li> <li>Gradio</li> </ul>"},{"location":"AI%5CML/index.html#computer-vision","title":"Computer vision","text":"<ul> <li>SAM3</li> <li>Label Studio</li> </ul>"},{"location":"AI%5CML/index.html#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li>Stable-Baselines3</li> <li>Gymnasium</li> <li>Hugging Face Stable Baselines 3</li> </ul>"},{"location":"AI%5CML/gradio.html","title":"Gradio","text":"<p>It is a fast way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere.</p>"},{"location":"AI%5CML/gymnasium.html","title":"Gymnasium","text":"<p>It is the new version of Gym library.</p>"},{"location":"AI%5CML/gymnasium.html#basic","title":"Basic","text":"<p>It provides two things:</p> <ul> <li>An interface that allows you to create RL environments</li> <li>A collection of environments(gym-control, atari,box2D...)</li> </ul> <p>With Gymnasium:</p> <ol> <li>Create our environment using: <code>gymnasium.make()</code></li> <li>reset the environment to its initial state with <code>observation = env.reset()</code></li> </ol> <p>At each step:</p> <ol> <li>Get an action using our model</li> <li>Using <code>env.step(action)</code>, we perform this acton in the environment and get<ul> <li>observation: The new state</li> <li>reward: The reward</li> <li>terminated: Indicates if the episode terminated</li> <li>truncated: Indicates a timelimit or if an agent go out of bounds of the environment for instance</li> <li>info: A dictionary that provides additional information(depend on the environment)</li> </ul> </li> </ol> <p>If the episode is terminated:</p> <ul> <li>we reset the environment to its initial state</li> </ul>"},{"location":"AI%5CML/hugging_face_hub.html","title":"Hugging Face Hub","text":"<p>It provides a seamless interface to the Hugging Face Hub, enabling developers to share, download, and manage machine learning models, datasets, and other artifacts in a centralized way</p>"},{"location":"AI%5CML/hugging_face_hub.html#functions","title":"Functions","text":"<ul> <li>Login to Hugging Face(jupyter notebooks): </li> </ul> <pre><code>    from huggingface_hub import login\n    login()\n</code></pre>"},{"location":"AI%5CML/hugging_face_sb3.html","title":"Hugging Face Stable Baselines 3","text":"<p>It is a library to load and upload stable-baselines3 models from the Hub with gymnasium and gymnasium compatible environments.</p>"},{"location":"AI%5CML/hugging_face_sb3.html#functions","title":"Functions","text":"<ul> <li>package_to_hub (model, model_name, model_architecture, env_id, eval_env, repo_id, commit_message)</li> <li>load_from_hub (repo_id, filename)</li> </ul>"},{"location":"AI%5CML/hugging_face_transformers.html","title":"Hugging Face Transformers","text":"<p>It is a popular open-source python library. It provides easy access to thousands of pre-trained models to do inference and training.</p>"},{"location":"AI%5CML/hugging_face_transformers.html#pipeline","title":"Pipeline","text":"<p>It is the most convenient way to inference with a pretrained model</p> <pre><code>from transformers import pipeline, infer_device\n\ndevice = infer_device() # infer_device() can automatically detect an available accelerator for inference\n\ntext_generator = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\", device=device)\n</code></pre>"},{"location":"AI%5CML/label_studio.html","title":"Label Studio","text":""},{"location":"AI%5CML/label_studio.html#label-studio-ml-backend","title":"Label Studio ML backend","text":"<p>How do we use model to help us predict annotations</p>"},{"location":"AI%5CML/matplotlib.html","title":"Matplotlib","text":""},{"location":"AI%5CML/matplotlib.html#basic","title":"Basic","text":"<p>Components of a Matplotlib Figure: </p>"},{"location":"AI%5CML/matplotlib.html#functions","title":"Functions","text":"<ul> <li><code>plt.tight_layout()</code>: Adjust the padding between and around subplots</li> <li><code>plt.xticks(rotation=n)</code>: rotate x-axis</li> <li><code>ax.tick_params(\"x\", labelrotation=n)</code>: rotate multiple subplots' x-axis</li> <li><code>plt.xlabel(\"name)</code>: set x label's name</li> <li><code>plt.title(\"title\")</code>: set plot title</li> </ul>"},{"location":"AI%5CML/mxnet.html","title":"MXNet","text":""},{"location":"AI%5CML/mxnet.html#architecture","title":"Architecture","text":"<ol> <li>Frontend: Python, R, Go, Scalar</li> <li>MXNet C++ kernel</li> <li>Backend: cuDNN, MKL</li> <li>Computing machine</li> </ol> <p>As of September 2023, it is no longer actively developed.</p>"},{"location":"AI%5CML/numpy.html","title":"Numpy","text":"<p>A python library that using C/Fortran back-end.  The reason why we use numpy to perform matrix operations 1. Python arrays allow any data type within a single array, meaning each element stores the memory address of that object. Consequently, these elements are often stored in disparate memory locations. In contrast, C arrays only permit homogeneous data types, and their elements are stored in continuous memory addresses. 2. Modern CPUs have SIMD instruction sets that allow a single instruction to operate on miltiple data point simultaneously. For example, instead of adding two numbers at a time, a CPU with SIMD can add four or eight numbers at once. Because Numpy arays store data contiguously and homogeneously, they are perfectly suited for these SIMD optimizations. The underlying C/Fortran code in NumPy can be compiled to take advantage of these SIMD instructions.</p>"},{"location":"AI%5CML/numpy.html#functions","title":"Functions","text":"<ul> <li><code>np.array()</code> - create an array</li> <li><code>np.dot()</code> - implement dot product operation</li> <li><code>np.random.permutation()</code> - randomly permute a sequence, or return a permuted range.</li> <li><code>np.argmin()</code>: return the indices o the minimum values along an axis</li> <li><code>np.mean()</code>: Compute the arithmetic mean along the specified axis.</li> <li><code>np.array_equal</code>: True if two arrays have the same shsape and elements, False otherwise.</li> </ul>"},{"location":"AI%5CML/numpy.html#tips","title":"Tips:","text":""},{"location":"AI%5CML/pandas.html","title":"Pandas","text":"<p>Pandas has two built-in data types: series and data frame - Series are columns in the dataset - Data Frame is the object the dataset to a data type that can be use in pandas</p>"},{"location":"AI%5CML/pandas.html#function","title":"Function:","text":""},{"location":"AI%5CML/pandas.html#initial-data-exploration","title":"Initial data exploration:","text":"<ul> <li><code>df = pd.read_csv(\"path_to_dataset\")</code>: load dataset</li> <li><code>df.info()</code>: what types are used</li> <li><code>df.describe()</code>: overview of the numerical data, gives the most important statistical analysis.</li> <li><code>df[column].isna()</code>: by checking which values are NaN, with True and False values.</li> <li><code>df.dropna(axis, how, subset, thresh)</code>: drop rows if any values is NaN</li> <li><code>df.fillna(value, method, axis, limit)</code>: replace NA with mean, max or min values of the column, strings and 0.</li> <li><code>df.nunique()</code>: how many unique values we have in each column of the Dataframe.</li> <li><code>df.duplicated()</code>: To search for entire rows of data that duplicate other rows, and return a mask</li> <li><code>pd.to_numeric(df[column], error={'ignore','raise','coerce'}, downcast={'integer','signed','unsigned','float'})</code>: ensure every entry in a column is numeric. </li> <li><code>df.groupby(\"series\")</code>: Splitting dataframe bu categorical value inside this series</li> </ul>"},{"location":"AI%5CML/pandas.html#explore-information-of-one-column","title":"Explore information of one column","text":"<ul> <li><code>pd.unique(column)</code>: Return unique values based on hash table.</li> <li><code>Series.value_counts()</code>: Return a Series containing counts of unique values</li> <li><code>Series.mean()</code>: Return the mean of the values over the requested axis</li> <li><code>Series.size()</code>: Return the number of elements in the underlying data</li> </ul>"},{"location":"AI%5CML/pandas.html#data-manipulation","title":"Data manipulation","text":"<ul> <li><code>pd.concat(df, axis, join)</code>:append one data structures to the other along one axis</li> <li><code>pd.merge(dataframe1. dataframe2. on, how)</code>: Merge two dataframes by one or multiple Keys/ Indices, A little bit like Join in SQL</li> <li></li> <li><code>df_filtered = df[df[\"Column\"].isin([specific value1, specific value2])]</code>: Filter rows that contain specifc values in a column:.</li> <li><code>df.drop('column name', axis=1)</code>: Remove column or columns<ul> <li>axis=1: Column operations</li> </ul> </li> <li><code>DataFrame.reset_index</code>: Change multiIndex back to a regular column and reset it to the default numeric index.</li> <li><code>Series.apply()</code>: Invoke function on values of Series</li> <li><code>pd.DataFrame(data={'col1':[1,2], 'col2':[3,4]})</code>: create dataframe </li> </ul>"},{"location":"AI%5CML/pandas.html#date","title":"Date","text":"<ul> <li><code>Series.dt.to_period(freq)</code>: Covert DatetimeArray to PeriodArray<ul> <li>freq: W, M, Q, Y</li> </ul> </li> </ul>"},{"location":"AI%5CML/pandas.html#fact","title":"Fact:","text":"<ul> <li>type(np.NaN) = float, which means coerce a column can apply all numeric operations to entire columns.</li> <li>mask is a series return True and False</li> <li>~ will revert boolean values in a series</li> </ul>"},{"location":"AI%5CML/pillow.html","title":"Pillow","text":"<p>It is a python imaging lbrary adds image processing capabilities to your python interpreter.</p>"},{"location":"AI%5CML/pytorch.html","title":"Pytorch","text":"<p>Pytorch follows imperative programming. You can compile the model by using <code>torch.jit.script</code>, which transforms python code into torchscript. The compiled model can be further optimized and run in an environment without a python interpreter, and you can save the whole model(code and parameters) to run on other platforms.</p>"},{"location":"AI%5CML/pytorch.html#architecture","title":"Architecture","text":"<ol> <li>Frontend: Python</li> <li>Pytorch C++ core</li> <li>Backend: cuDNN, MKL</li> <li>Hardware Interface Layer: CUDA runtime, CPU instructions</li> <li>Computing Hardware: Nvidia GPU, CPU</li> </ol> <p>Python Frontend thread will wait for the C++ backend thread to finish computing the result. One benefit of this design is that the Python frontend thread does not need to perform actual computation. Thus, there is little impact on the program's overall performance, regardless of Python's performance.</p>"},{"location":"AI%5CML/pytorch.html#torch","title":"Torch","text":"<ul> <li> <p>torch.tensor() - construct a tensor by supplying the exact values for each element by supplying python list containing numerical literals</p> </li> <li> <p>torch.arange() - return a 1-d tensor, with values from the interval <code>[start, end)</code> taken with common difference <code>step</code> beginning from start</p> </li> <li> <p>torch.range() - same as torhc.arange, except include end in values</p> </li> <li> <p>torch.numel(x) / x.numel() - return the total number of elements in the input tensor</p> </li> <li> <p>torch.reshape(x,shape) / x.reshape(shape) - Returns a tensor with the same data and number of elements as <code>input</code>, but with the specified shape</p> </li> <li> <p>torch.zeros() - Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument <code>size</code></p> </li> <li> <p>torch.ones() - Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument <code>size</code></p> </li> <li> <p>torch.randn() - Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution)</p> </li> <li> <p>torch.normal() - same as randn(), except for customize mean and variance</p> </li> <li> <p>torch.rand() - Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)</p> </li> <li> <p>torch.randint() - Returns a tensor filled with random integers generated uniformly between <code>low</code> (inclusive) and <code>high</code> (exclusive)</p> </li> <li> <p>torch.exp(x) - Returns a new tensor with the exponential of the elements of the input tensor <code>input</code></p> </li> <li> <p>torch.cat() - Concatenates the given sequence of tensors in <code>tensors</code> in the given dimension</p> </li> <li> <p>torch.sum(x) / x.sum() - Returns the sum of all elements in the <code>input</code> tensor</p> </li> <li> <p>torch.t(x) - Expects input to be &lt;= 2-D tensor and transposes dimensions 0 and 1</p> </li> <li> <p>torch.clone(x) / x.clone() - Returns a copy of <code>input</code> by allocating new memory</p> </li> <li> <p>torch.mean(x) / x.mean() - Returns the mean value of all elements in the <code>input</code> tensor. Input must be floating point or complex</p> </li> <li> <p>torch.cumsum(x,dim) / x.cumsum(dim) - Returns the cumulative sum of elements of <code>input</code> in the dimension <code>dim</code></p> </li> <li> <p>torch.dot() - Computes the dot product of two 1D tensors</p> </li> <li> <p>torch.mv() - Performs a matrix-vector product of the matrix <code>input</code> and the vector <code>vec</code></p> </li> <li> <p>torch.mul() - Multiplies <code>input</code> by <code>other</code> by Hadamard Product</p> </li> <li> <p>torch.matmul() / @ - Matrix product of two tensors</p> </li> <li> <p>torch.mm - same as torch.matmul, but without boardcast</p> </li> <li> <p>torch.norm() - Returns the matrix norm or vector norm of a given tensor. Find L2 norm</p> </li> <li> <p>torch.abs(x).sum() - Return L1 norm</p> </li> <li> <p>torch.no_grad - Context-manager that disables gradient calculation</p> </li> <li> <p>torch.save - Saves an object to a disk file</p> </li> <li> <p>torch.load - Loads an object saved with torch.save from a file</p> </li> <li> <p>torch.squeeze(input) - Returns a tensor with all specified dimensions of input of size 1 removed.</p> </li> </ul> <p>Tips:</p> <ol> <li>To automatically infer one component of the shape, we can place a <code>-1</code> for the shape component that should be inferred automatically. In our case, instead of calling <code>x.reshape(3, 4)</code>, we could have equivalently called <code>x.reshape(-1, 4)</code> or <code>x.reshape(3, -1)</code>.</li> <li>boardcasting</li> <li>saving memory<ol> <li>Running operations can cause new memory to be allocated to host results. For example, if we write <code>Y = X + Y</code>, we dereference the tensor that <code>Y</code> used to point to and instead point <code>Y</code> at the newly allocated memory.</li> <li>Fortunately, performing in-place operations is easy. We can assign the result of an operation to a previously allocated array <code>Y</code> by using slice notation: <code>Y[:] = &lt;expression&gt;</code>.</li> <li>conversion to other python objects:<ol> <li>Converting to a NumPy tensor (<code>ndarray</code>), or vice versa, is easy. The torch tensor and NumPy array will share their underlying memory, and changing one through an in-place operation will also change the other.</li> </ol> </li> </ol> </li> </ol>"},{"location":"AI%5CML/pytorch.html#torchtensor","title":"Torch.Tensor","text":"<ul> <li> <p>Tensor.shape - Returns the size of the <code>self</code> tensor</p> </li> <li> <p>Tensor.T - Returns a view of this tensor with its dimensions reversed</p> </li> <li> <p>Tensor.size() - Returns the size of the <code>self</code> tensor</p> </li> <li> <p>Tensor.requires_grad - Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise</p> </li> <li> <p>Tensor.requires_grad_ - Change if autograd should record operations on this tensor: sets this tensor's <code>requires_grad</code> attribute in-place</p> </li> <li> <p>Tensor.grad - This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <code>backward()</code> computes gradients for <code>self</code></p> </li> <li> <p>Tensor.backward() - Computes the gradient of current tensor wrt graph leaves</p> </li> <li> <p>Tensor.detach() - Returns a new Tensor, detached from the current graph. The result will never require gradient</p> </li> </ul>"},{"location":"AI%5CML/pytorch.html#torchnn","title":"Torch.nn","text":"<ul> <li>Tensor.nn.Parameter - A kind of Tensor that is to be considered a module parameter</li> </ul>"},{"location":"AI%5CML/pytorch.html#torchnnmodule","title":"Torch.nn.Module","text":"<ul> <li> <p>net.state_dict() - Return a dictionary containing references to the whole state of the module</p> </li> <li> <p>net.load_state_dict() - Copy parameters and buffers from state_dict into this module and its descendants</p> </li> </ul>"},{"location":"AI%5CML/pytorch.html#torchcuda","title":"Torch.cuda","text":"<ul> <li>torch.cuda.is_available: check if GPU is available</li> <li>torch.cuda.get_device_name(0): Get names of GPU available</li> <li>torch.cuda.synchronize - Wait for all kernels in all streams on a CUDA device to complete.</li> </ul>"},{"location":"AI%5CML/sam3.html","title":"SAM3","text":"<p>SAM stands for Segmentation Angthing model, it's the SOTA Segmentation model, made by meta.</p>"},{"location":"AI%5CML/sam3.html#basic-usage","title":"Basic usage","text":"<p>Find in their official github repo</p>"},{"location":"AI%5CML/seaborn.html","title":"Seaborn","text":"<p>A high level abstract of matplotlib, with built-in data type Series and DataFrame</p>"},{"location":"AI%5CML/seaborn.html#parameter","title":"Parameter","text":"<ul> <li>data: Tidy dataframe where each column is a variable and each row is an observation</li> <li>hue: Variable in data to map plot aspects to different colors</li> <li>row, col: Categorical variale that will determine the faceting of the grid</li> <li>kind: The kind of plot to draw</li> <li>col_wrap: how many subplot shows in one row</li> <li>height: height of each subplot</li> <li>aspect: Aspect ratio of each facet</li> <li>errorbar: a line that represents the variability or uncertainty around a point estimate on a graph, like a mean or median</li> <li>legend: \"auto\",\"brief\", \"full\" or False</li> <li>legend_out: bool, if true, the figure size will be extended, and legend will be drawn outside the plot.</li> <li>palette: colors to use for the different levels of the <code>hue</code> variable</li> </ul>"},{"location":"AI%5CML/seaborn.html#plots","title":"Plots","text":"<ul> <li><code>sns.catplot()</code>: a quick way to generate multi-plot figure</li> <li><code>sns.pairplot(data)</code>: used to find correlation between each variable.</li> <li><code>sns.lineplot</code></li> </ul>"},{"location":"AI%5CML/seaborn.html#functions","title":"Functions:","text":"<ul> <li><code>g.set_xticklabels(labels=[a,b,c], rotation=n)</code>: rotate x_tick in seaborn, set labels. </li> </ul>"},{"location":"AI%5CML/sklearn.html","title":"SciKit Learn","text":"<p>It is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities.</p>"},{"location":"AI%5CML/sklearn.html#functions","title":"Functions","text":""},{"location":"AI%5CML/sklearn.html#models","title":"Models","text":"<ul> <li>`sklearn.cluster.KMeans(n_cluster)</li> <li>`sklearn.neighbors.KNeighborsClassifier(n_neighbors,)</li> </ul>"},{"location":"AI%5CML/sklearn.html#metrics","title":"Metrics","text":"<ul> <li><code>sklearn.metrics.pairwise_distances()</code>: Compute the distance matrix from a feature arary X and optional Y, by default \"Euclidean distance\".</li> <li><code>sklearn.metrics.accuracy_score(y_true, y_pred)</code>: Accuracy classification score.</li> </ul>"},{"location":"AI%5CML/sklearn.html#preprocessing","title":"Preprocessing","text":"<ul> <li><code>sklearn.preprocessing.StandardScalar()</code>: Standardize feature by removing the mean and scaling to unit variance<ul> <li>Usually we define a scaler as <code>sklearn.preprocessing.StandardScalar()</code>, because we hope to use the same scaler in both training data and test data</li> <li>then we use <code>scaler.fit(data)</code> to calculate mean and unit variance, which will save in scaler</li> <li>finally we transform our data by using <code>scaler.transform(data)</code></li> <li>you can also directly using <code>scaler.fit_transform(data)</code></li> </ul> </li> </ul>"},{"location":"AI%5CML/sklearn.html#model-selection","title":"Model selection","text":"<ul> <li><code>sklearn.model_selection.train_test_split(*arrays, train_size, test_size)</code>: Split arrays or matrices into random train and test subsets.</li> <li><code>sklearn.model_selection.cross_val_score(estimator, X, y, cv, scoring=\"accuracy\")</code>: Evaluate a score by cross-validation.</li> </ul>"},{"location":"AI%5CML/stable_baselines3.html","title":"Stable Baselines3","text":""},{"location":"AI%5CML/stable_baselines3.html#basic-usage","title":"Basic usage","text":"<ol> <li>Create environment: gym.make() / make_vec_env()</li> <li>Define the model, instantiate this model: <code>model = PPO(\"MlpPolicy)</code></li> <li>Train the agent by define the traning timesteps: <code>model.learn(total_timesteps=int(n))</code></li> <li>Wrap the environment in a Monitor: <code>eval_env = Monitor(env)</code></li> <li>Evaluate policy: <code>evaluate_policy(model, eval_env, m_eval_episodes, deterministics)</code></li> </ol>"},{"location":"AI%5CML/stable_baselines3.html#functions","title":"Functions","text":""},{"location":"AI%5CML/stable_baselines3.html#training","title":"Training","text":"<ul> <li><code>make_vec_env(\"EnvName\", n_envs=n)</code>: Create a vectorized environment of n environment, a method for stacking multiple independent environment into a single environment.</li> <li><code>model.save(\"filename\")</code>: Specify file name for model and save the model to file</li> </ul>"},{"location":"AI%5CML/stable_baselines3.html#evaluation","title":"Evaluation","text":"<ul> <li><code>evaluate_policy(model, env, n_eval_episodes, deterministics)</code>: Run the policy foe n_eval_episodes and outputs the average return per episode.<ul> <li>deterministics: Whether to use deterministic or stochastic actions</li> </ul> </li> <li><code>Monitor</code>: It is used to know the episode reward, length, time and other data</li> </ul>"},{"location":"AI%5CML/stable_baselines3.html#load-model","title":"Load model","text":"<ul> <li><code>methods.load(checkpoint, custom_objects=custome_objects, print_system_info=True)</code>: models from huggingface hub<ul> <li>checkpoint: <code>huggingface_sb3.load_from_hub()</code></li> <li>custom_objects: Dictionary of objects to replace upon loading, such as learning rate</li> </ul> </li> </ul>"},{"location":"AI%5CML/ultralytics.html","title":"Ultralytics","text":""},{"location":"AI%5CML/ultralytics.html#functions","title":"Functions","text":""},{"location":"Hardware/index.html","title":"Hardware","text":""},{"location":"Hardware/index.html#nvidia-hardware-ecosystem","title":"Nvidia Hardware Ecosystem","text":"<p>Home Page</p> <ul> <li>Nvidia Hardware Terminology</li> <li>Nvidia GPU Architecture Evolution</li> <li>Nvidia Product line Overview</li> <li>Nvidia Embedded System Product Line Overview</li> </ul> <p>If you also interested in Nvidia's software ecosystem: Nvidia Software Ecosystem</p>"},{"location":"Hardware/index.html#configuration","title":"Configuration","text":"<ul> <li>Jetson</li> </ul>"},{"location":"Hardware/index.html#parallel-computation-in-multi-gpus","title":"Parallel Computation in Multi-GPUs","text":"<ul> <li>Single-node Multi-GPU</li> <li>Multi-node Multi-GPU</li> <li>Model Parallelism</li> <li>Data Parallel</li> <li>Distributed Data Parallel</li> <li>Parallel Computation demo</li> </ul>"},{"location":"Hardware/data_parallel.html","title":"Data Parallel","text":"<p>You need to make sure the model parallelism method is data parallelism. This is an old way to do data parallel in pytorch</p> <p>Learn what is model parallelism </p>"},{"location":"Hardware/data_parallel.html#steps","title":"Steps","text":"<p>Suppose we have 4 GPUs</p> <ul> <li>Step 1: GPU 0 scatters data and model<ul> <li>GPU 0 \u2192 GPU 1: copy model + 1/4 batch</li> <li>GPU 0 \u2192 GPU 2: copy model + 1/4 batch</li> <li>GPU 0 \u2192 GPU 3: copy model + 1/4 batch</li> </ul> </li> <li>Step 2: Each GPU computes independently<ul> <li>GPU 0: forward + backward \u2192 gradient_0</li> <li>GPU 1: forward + backward \u2192 gradient_1</li> <li>GPU 2: forward + backward \u2192 gradient_2</li> <li>GPU 3: forward + backward \u2192 gradient_3</li> </ul> </li> <li>Step 3: All gradients sent back to GPU 0<ul> <li>GPU 1 \u2192 GPU 0: send gradient_1</li> <li>GPU 2 \u2192 GPU 0: send gradient_2</li> <li>GPU 3 \u2192 GPU 0: send gradient_3</li> </ul> </li> <li>Step 4: GPU aggregates and updates<ul> <li>GPU 0: average gradients + update parameters</li> </ul> </li> <li>Step 5: GPU 0 broadcasts new model<ul> <li>GPU 0 \u2192 GPU 1, 2, 3: updated parameters</li> </ul> </li> </ul> <p>This is an AllReduce step.</p>"},{"location":"Hardware/data_parallel.html#problems","title":"Problems","text":"<ul> <li>Assume the file size of the parameters in a model is M. GPU0 is required to receive 3M of data. GPU0 is also required to send 3M of data. GPU0's bandwidth for both sending and receiving files is fully utilized, while GPU1, GPU2, and GPU3's bandwidth is not fully utilized (only 1/3 compared to GPU0).  </li> <li>Do Not support Muti-node Multi-GPUs</li> </ul> <p>Learn what is single-node multi-GPUs Learn what is multi-node multi-gpus</p>"},{"location":"Hardware/distributed_data_parallel.html","title":"Distributed Data Parallel","text":"<p>Learn what is Data Parallel</p> <p>This is the modern pytorch method to do data parallel. which is a ring synchronization method. NCCL provides good optimization for DDP.</p>"},{"location":"Hardware/distributed_data_parallel.html#steps","title":"Steps:","text":"<ul> <li> <p>Assume we have 4 nodes</p> </li> <li> <p>Broke the gradients into n chunks</p> </li> <li>and then doing steps below </li> <li> <p>Untile every GPU's n chunks has four red cross.</p> </li> <li> <p>Then the total time improve from DP's n to (n-1)/n which is 1</p> </li> <li> <p>This is an Ring-AllReduce step.</p> </li> <li>Support Multi-node well</li> </ul> <p>Learn what is single-node multi-GPUs Learn what is multi-node multi-gpus</p>"},{"location":"Hardware/jetson.html","title":"Jetson","text":"<p>Always check your JetPack version first - Jetson runs on ARM architecture, so NVIDIA only provides specific framework versions for each JetPack release.</p>"},{"location":"Hardware/jetson.html#the-torchvision-problem","title":"The TorchVision Problem","text":"<p>NVIDIA provides PyTorch for Jetson, but not TorchVision. If you try compiling TorchVision from source, you'll likely hit:</p> <pre><code>RuntimeError: operator torchvision::nms does not exist\n</code></pre>"},{"location":"Hardware/jetson.html#solution","title":"Solution","text":"<p>Use pre-compiled torchvision that provided by ultralytics.  Firstly, check your</p> <ul> <li>Python version</li> <li>PyTorch version</li> <li>JetPack version</li> </ul> <p>Then, find matches torchvision from Ultralytics releases with their v0.0.0 assets.</p>"},{"location":"Hardware/model_parallelism.html","title":"Model Parallelism","text":"<p>Here are some common methods about how to achieve model parallelism in multi-gpus </p> <p>Nowadays, we use data parallelism more; the other two are suitable for small VRAM GPUs to train a large network.</p>"},{"location":"Hardware/multi_node_multi_gpu.html","title":"Multi-Node Multi-GPU","text":""},{"location":"Hardware/multi_node_multi_gpu.html#properties","title":"Properties","text":"<p>Pros:</p> <ul> <li>GPUs are distributed across multiple machines</li> <li>Within the node: NVLink</li> <li>Between nodes: Networks(Ethernet, InfiniBand)</li> <li>Strong scalability</li> <li>Cost is relative flexible</li> </ul> <p>Cons:</p> <ul> <li>Lower Bandwidth</li> <li>Higher Latency</li> <li>May exist network issue</li> </ul> <p>Examples:</p> <ul> <li>Cloud-based training cluster </li> </ul>"},{"location":"Hardware/parallel_compt_demo.html","title":"Parallel computation demo","text":"<p>An illustration of the computational graph and its dependencies for a simple two-layer MLP when training on a CPU and two GPUs, </p>"},{"location":"Hardware/single_node_multi_gpu.html","title":"Single-Node Multi-GPU","text":""},{"location":"Hardware/single_node_multi_gpu.html#properties","title":"Properties:","text":"<p>Pros:</p> <ul> <li>All GPUs are in the same machine</li> <li>Communicate with each other by NVLink or PCIe</li> <li>Super high Bandwidth(300-600GB/s)</li> <li>Super low latency(&lt;1 us)</li> </ul> <p>Cons:</p> <ul> <li>Limited number of GPU</li> <li>High Cost to configure one machine</li> </ul> <p>Examples:</p> <ul> <li>DGX A100</li> <li>4 RTX 4090 Workstation</li> </ul>"},{"location":"Hardware/Nvidia/index.html","title":"Nvidia Hardware Ecosystem","text":"<ul> <li>Nvidia Hardware Terminology</li> <li>Nvidia GPU Architecture Evolution</li> <li>Nvidia Product line Overview</li> <li>Nvidia Embedded System Product Line Overview</li> </ul>"},{"location":"Hardware/Nvidia/nvidia_embedded_system.html","title":"Nvidia Embedded System","text":""},{"location":"Hardware/Nvidia/nvidia_embedded_system.html#nvidia-hardware-terminology","title":"Nvidia Hardware Terminology","text":"<p>See in this page: Nvidia Hardware Terminology</p>"},{"location":"Hardware/Nvidia/nvidia_embedded_system.html#jetson-series","title":"Jetson series","text":"<p>It is an edge AI computing platform, within development board form.</p> <ul> <li>Jetson Nano: for beginner<ul> <li>128 cuda core</li> <li>4 core ARM CPU</li> <li>4GB  RAM</li> <li>0.5 TOPS(FP16)</li> <li>JetPack 4.6(Ubuntu 18.04)</li> <li>CUDA 10.2</li> <li>TensorRT7</li> </ul> </li> <li>Jetson Orin Nano: for small projects<ul> <li>1024 cuda cores</li> <li>32 tensor cores</li> <li>6 core ARM CPU</li> <li>8GB RAM</li> <li>67 TOPS</li> <li>JetPack6(Ubuntu 22.04)</li> <li>CUDA 12.2</li> <li>TensorRT 8.6</li> <li>Pytorch 2.0, TensorFlow 2.x</li> </ul> </li> <li>Jetson Orin NX:<ul> <li>1024 cuda cores</li> <li>64 tensor coresB RAM</li> <li>100 TOPS</li> </ul> </li> <li>Jetson AGX Orin<ul> <li>2048 cuda cores</li> <li>64 tensor cores</li> <li>12 core ARM CPU</li> <li>275 TOPS</li> </ul> </li> <li>Jetson AGX Thor<ul> <li>2560 cuda cores</li> <li>96 tensor cores</li> <li>14 core ARM CPU</li> <li>2070 TFLOPS(FP4 Sparse)</li> </ul> </li> </ul>"},{"location":"Hardware/Nvidia/nvidia_embedded_system.html#drive-agx","title":"Drive AGX","text":"<p>NVIDIA DRIVE AGX gives you a scalable and energy-efficient AI computing platform designed to process the complex workloads required for autonomous driving.</p>"},{"location":"Hardware/Nvidia/nvidia_embedded_system.html#hardware","title":"Hardware","text":"<ul> <li>Nvidia Drive AGX Hyperion 10<ul> <li>2 x Drive AGX Thor</li> <li>Features 14 high-definition cameras, 9 radars, 1 lidar, and 12 ultrasonic sensors and a microphone array.</li> <li>Includes DRIVE\u2122 AV software, purpose-built for L4 autonomy.</li> </ul> </li> <li>Nvidia Drive AGX Thor<ul> <li>Delivers more than 1,000 INT8 TOPS (2,000 FP4 FLOPs).</li> <li>Offers a scalable architecture supporting Level 2+ to fully autonomous driving.</li> <li>Provides superior safety, with ASIL-D compliance and redundancy.</li> </ul> </li> <li>Nvidia Drive AGX Orin<ul> <li>Delivers up to 254 TOPS of AI performance.</li> <li>Offers a scalable architecture supporting Level 2+ to fully autonomous driving.</li> <li>Provides superior safety, with ASIL-D compliance and redundancy</li> </ul> </li> </ul>"},{"location":"Hardware/Nvidia/nvidia_embedded_system.html#software","title":"Software","text":"<ul> <li>Nvidia DriveOS</li> <li>Nvidia Drive AV</li> </ul>"},{"location":"Hardware/Nvidia/nvidia_embedded_system.html#clara-agx-ai-for-medical-devices-and-robotics","title":"Clara AGX: AI for Medical Devices and Robotics","text":""},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html","title":"Nvidia GPU Architecture Evolution","text":""},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html#nvidia-hardware-terminology","title":"Nvidia Hardware Terminology","text":"<p>See in this page: Nvidia Hardware Terminology</p>"},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html#timeline","title":"Timeline","text":"<ul> <li>2016 -- Pascal</li> <li>2017 -- Volta</li> <li>2018 -- Turing</li> <li>2020 -- Ampere</li> <li>2022 -- Hopper</li> <li>2024 -- Blackwell</li> <li>2025+ -- Rubin</li> </ul>"},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html#pascal","title":"Pascal","text":"<p>Introduce HBM2, NVLink1.0(160GB/s), Unified memory architecture</p> <p>Representitive products:</p> <ul> <li>GTX 1060, 1070 and 1080ti</li> <li>Tesla P100</li> <li>Quadro P6000</li> </ul> <p>Those cards doesn't have Tensor Cores. Therefore, they performed poorly during AI training.</p>"},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html#volta","title":"Volta","text":"<p>Introduce Tensor Cores 1.0, NVLink 2.0(300GB/s)</p> <p>Representitive products:</p> <ul> <li>Tesla V100<ul> <li>16G/32G HBM2</li> <li>FP16 performance: 125 TFLOPS</li> </ul> </li> </ul>"},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html#turing","title":"Turing","text":"<p>Introduce Tensor Cores 2.0(Append INT8, INT4), RT Cores, DLSS</p> <p>Representitive Products:</p> <ul> <li>RTX 2060, 2070 and 2080ti</li> <li>GTX 1660</li> <li>Tesla T4</li> <li>Quadro RTX 6000, 8000</li> </ul>"},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html#ampere","title":"Ampere","text":"<p>Introducr Tensor Cores 3.0(Append TF32, BF16 and FP64), Structured Sparsity(2:4), MIG, NVLink 3.0(600GB/s)</p> <p>Representitive Products:</p> <ul> <li>A100(40GB/80GB HBM2e)<ul> <li>Cuda cores: 6912</li> <li>Tensor cores: 432 (3.0)</li> <li>NVLink 3.0</li> <li>Memory bandwidth: 2039GB/s </li> <li>312 TFLOPS(FP16)</li> <li>156 TFLOPS(TF32)</li> <li>624 TFLOPS(INT8 Sparse)</li> </ul> </li> <li>RTX 3060, 3070, 3080</li> <li>RTX3090<ul> <li>CUDA cores: 10496</li> <li>Tensore cores: 328</li> <li>Memory: 24G</li> <li>285 TFLOPS(FP16)</li> </ul> </li> </ul>"},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html#hopper","title":"Hopper","text":"<p>Introduce Tensor COres 4.0(Append FP8), Transformer Engine(Dynamically switch FP8/FP16), NVLink 4.0(900GB/s), DPX, Tensor Memory Accelerator, Grace CPU</p> <p>Representitive Products:</p> <ul> <li>H100(80GB HBM3)<ul> <li>Cuda cores: 18432</li> <li>Tensor cores: 640(4.0)</li> <li>NVLink 4.0</li> <li>Memory Bandwidth: 3350GB/s</li> <li>1979 TFLOPS(FP8, Transformer Engine)</li> <li>990 TFLOPS(FP16)</li> <li>60 TFLOPS(FP64)</li> <li>3958 TOPS(INT8 Sparse)</li> </ul> </li> <li>H200(141GB HBM3e)<ul> <li>Memory Bandwidth: 4800GB/s</li> <li>141GB HBM3e memory</li> <li>Others are the same as H100</li> </ul> </li> <li>Grace CPU<ul> <li>Traditional bottleneck: GPU-CPU communication<ul> <li>Intel/AMD CPU &lt;- PCIe 5.0(128GB/s) -&gt; NVIDIA GPU</li> </ul> </li> <li>NVLink-C2C(chip-to-chip)<ul> <li>Grace CPU &lt;- NVLink-C2c(9000GB/s) -&gt; Hopper GPU</li> </ul> </li> </ul> </li> </ul>"},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html#adalovelace","title":"AdaLovelace","text":"<p>Introduce Tensor Cores 4.0(consumer grade), DLSS 3.0, RT Core 3.0, GDDR6X upgrade</p> <p>Representitive Products:</p> <ul> <li>RTX 4070, 4080</li> <li>RTX 4090<ul> <li>Cuda Cores: 16384</li> <li>Tensor Cores: 512 (4.0)</li> <li>RT Core: 128</li> <li>24GB GDDR6X memory</li> <li>1008GB/s memory bandwidth</li> <li>661 TFLOPS(FP16 Sparse)</li> <li>83 TFLOP(FP32)</li> <li>1321 TOPS(INT8)</li> </ul> </li> <li>RTX 6000 Ada</li> <li>L40s<ul> <li>Cuda Cores: 18176</li> <li>Tensor Cores: 568 (4.0)</li> <li>RT Cores: 142</li> <li>142 GDDR6X(with ECC) memory</li> <li>864GB/s memory bandwidth</li> </ul> </li> </ul>"},{"location":"Hardware/Nvidia/nvidia_gpu_architecture.html#blackwell","title":"Blackwell","text":"<p>Introduce Tensor Cores 5.0(Append FP4/FP6), Transformer Engine 2.0, NVLink 5.0(1800GB/s), 2 GPU designs, Micro-Tensor scaling, DLSS 4.0</p> <p>Representitive Products:</p> <ul> <li>B100</li> <li>B200(192GB HBM3e)<ul> <li>2 GPU cips</li> <li>Tensor Cores: Not release yet</li> <li>Memory Bandwidth: 8000GB/s</li> <li>NVLink 5.0</li> <li>18000 TFLOPS(FP4)</li> <li>9000 TFLOPS(FP8)</li> <li>4500 TFLOPS(FP16)</li> <li>72 PFLOPS(FP4, whole rack)</li> </ul> </li> <li>GB200(Grace-Blackwell)<ul> <li>1 Grace CPU(72 core ARM)</li> <li>2 B200 GPU</li> <li>NVLink-C2C(900GB/s)</li> <li>Whole memory: 192+480(CPU)</li> </ul> </li> <li>RTX 5090<ul> <li>CUDA cores: 21760</li> <li>Tensor Cores: 680 (5.0)</li> <li>RT cores 170 (4.0)</li> <li>32G GDDR7 memory</li> <li>Memory Bandwidth: 1792 GB/s</li> <li>3300 TOPS(FP4)</li> <li>900 TFLOPS(FP8)</li> <li>DLSS 4.0</li> </ul> </li> <li>RTX 5070, 5080</li> </ul>"},{"location":"Hardware/Nvidia/nvidia_hardware_term.html","title":"Nvidia Hardware Term","text":"<ul> <li>HBM: High Bandwidth Memory, a high-performance memory technology. <ul> <li>Traditional way: GPU -&gt; PCB traces(long) -&gt; GDDR memory chips(distributed across the PCB)</li> <li>HBM way: GPU -&gt; Interposer(very short) -&gt; HBM stack(close to the GPU)</li> </ul> </li> <li>NVLink: High-speed GPU interconnect technology is used to solve communcation bottlenecks in multi-GPU systems.<ul> <li>Allow multiple GPUs to directly transfer data to each other without going through the CPU or PCIe bus</li> <li>The bandwidth far exceeds PCIe</li> </ul> </li> <li>Cuda Core: General-purpose parallel processing unit, it can perform a wide range of calculation.</li> <li>Tensor Core: Specifically designed to accelerate matrix operations in deep learning. Each tensor core only support one precision matrix such as FP32/ FP64. E.g.: C = A * B (4*4 matrix)<ul> <li>CUDA Core matrix operaions<ul> <li>64 times matrix multiplication operation</li> <li>48 times matrix addition operation</li> <li>112 times commands</li> <li>several clock period</li> </ul> </li> <li>Tensor core matrix operations<ul> <li>1 specific command</li> <li>1 clock period</li> </ul> </li> </ul> </li> <li>TOPS: tera operations per second (INT8)</li> <li>TFLOPS: tera floating-point operations per second (FP16)</li> <li>DLSS: Use AI to upscale low-resolution images to high resolution. Training neural network to \"fill in the details\"</li> <li>RT Core: Specifically designed to accelerate ray tracing calculations.</li> <li>Structured Sparsity: Regularly removing weights allows the hardware to accelerate efficiently</li> <li>MIG: Divide a large GPU into multiple smaller GPUs, each running independently. MIG enables team members to work simultaneously by partitioning a single GPU into independent instances, each allocated to an individual user.</li> <li>Transformer Engine: Hardware acceleration engine specifically optimized for Transformer models<ul> <li>Automatically switch between FP8 and FP16</li> <li>Optimize Self-Attention operation</li> </ul> </li> <li>Tensor Memory Accelerator: Dedicated hardware unit optimizes data movement between GPU memory levels</li> <li>DPX: Dynamic Programming Instructions</li> <li>ECC: Error-Correcting Code, Memory error detection and correction technology, automatically detects and repairs bit flip errors in memory, but it will shrink 12.5% of memory</li> <li>Micro-Tensor scaling: Fine-grained Tensor quantization scaling techniques (related to FP8 training) since FP8 has a small dynamic range and is prone to overflow/underflow.<ul> <li>Tradition scaling: The entire tensor uses a scaling factor</li> <li>Per Block scaling: Tensor is divided into blocks, and each block is scaled independently</li> </ul> </li> <li>SXM and NVLink-C2C: SXM is used to communicate between Intel/AMD CPU and Nvidia GPU by using PCIe, but it's too slow, 180GB/s. Nvidia investigates Grace CPU which support NVLink-C2C, which support 9000GB/s.</li> </ul>"},{"location":"Hardware/Nvidia/nvidia_product.html","title":"Nvidia Product Line Category","text":""},{"location":"Hardware/Nvidia/nvidia_product.html#nvidia-hardware-terminology","title":"Nvidia Hardware Terminology","text":"<p>See in this page: Nvidia Hardware Terminology</p>"},{"location":"Hardware/Nvidia/nvidia_product.html#dgx-series-ai-super-computer","title":"DGX series - AI super computer","text":"<p>It is a series that hardware, software and network all configured by Nvidia, and with Nvidia directly supports</p> <ul> <li> <p>DGX A100:</p> <ul> <li>8 x A100(80GB) SXM</li> <li>NVLink 3.0 + NVSwitch</li> <li>2 x AMD EPYC 7742(128 cores)</li> <li>640G VRAM</li> <li>2TB DDR4 RAM</li> <li>15T NVMe4 SSD</li> <li>8 * 200GB/s HDR InfiniBand/Ethernet</li> <li>5 PFLOPS(FP16)</li> <li>DGX OS</li> </ul> </li> <li> <p>DGX H100:</p> <ul> <li>8 x H100(80GB) SXM5</li> <li>NVLink 4.0 + NVSwitch</li> <li>2 x Intel Xeon Platinum 8480C (112\u6838)</li> <li>640G VRAM</li> <li>2TB DDR5 RAM</li> <li>30T NVMe5 SSD</li> <li>8 * 400GB/s HDR InfiniBand</li> <li>5 PFLOPS(FP16)</li> </ul> </li> <li> <p>DGX H200:</p> <ul> <li>8 x H200(141GB) SXM5</li> <li>NVLink 4.0 + NVSwitch</li> <li>2 x Intel Xeon Platinum 8480C (112\u6838)</li> <li>1.1T VRAM</li> <li>2TB DDR5 RAM</li> <li>30T NVMe5 SSD</li> <li>8 * 400GB/s HDR InfiniBand</li> <li>5 PFLOPS(FP16)</li> </ul> </li> <li> <p>DGX B200</p> <ul> <li>8 x B200(192GB) HBM3e</li> <li>1.5T VRAM</li> <li>72 PFLOPS (FP4)</li> </ul> </li> <li> <p>DGX GB200</p> <ul> <li>72 x B200 GPU</li> <li>36 x Grace CPU</li> <li>12.8T HBM3e VRAM</li> <li>17 TB RAM</li> </ul> </li> <li> <p>DGX Station</p> <ul> <li>DGX Station A100: 4 * A100(40GB) with PCIe</li> <li>DGX Station(Blackwell): Based on GB300, 784GB Unified memory</li> <li>DGX Spark: GB10 Grace Blackwell, 128GB Unified memory</li> <li>1 PFLOPS(FP4)</li> </ul> </li> </ul>"},{"location":"Hardware/Nvidia/nvidia_product.html#hgx-series-oem-gpu-module","title":"HGX series - OEM GPU module","text":"<p>It is a series that Nvidia only supports motherboards and GPUs, but CPU, memory, storage, case and software need to support by OEM. It's 40%~60% cheaper than DGX</p> <ul> <li>HGX A100</li> <li>HGX H100</li> <li>HGX H200</li> <li>HGX B100</li> <li>HGX B200</li> </ul> <p>The GPUs configurations is the same as DGX series by each of them, but other components will depending on the OEM.</p>"},{"location":"Hardware/Nvidia/nvidia_product.html#igx-series-industrial-edge-ai","title":"IGX series - Industrial edge AI","text":"<p>It is a series of reliability-first edge AI computing platform. Such as medical imaging, Industrial testing, Logistics automation</p> <ul> <li>Nvidia IGX T7000</li> <li>Nvidia IGX T5000</li> <li>Nvidia IGX Orin 700</li> <li>Nvidia IGX Orin 500</li> </ul>"},{"location":"Hardware/Nvidia/nvidia_product.html#mgx","title":"MGX","text":"<p>It is a modular reference architecture, not a series, a kind of design standard.</p>"},{"location":"Hardware/Nvidia/nvidia_product.html#ovx-series-omniverse-super-workstation","title":"OVX series - Omniverse super workstation","text":"<p>It is a GPU system optimized for 3D design and digital twins. Such as building design, autonomous driving simulation, movie special effects</p> <ul> <li>Single Node OVX: 8 * L40s</li> <li>OVX superPOD(32 nodes): 256 * L40s</li> </ul>"},{"location":"Hardware/Nvidia/nvidia_product.html#nvidia-embedded-system-series","title":"Nvidia Embedded System series","text":"<p>Nvidia Embedded System Product Line Overview</p>"},{"location":"Hardware/Nvidia/nvidia_product.html#igx-vs-agx","title":"IGX vs AGX","text":"<p>AGX series:</p> <ul> <li>For developers and research</li> <li>Development board form factor</li> <li>Ease of use prioritized</li> </ul> <p>IGX series:</p> <ul> <li>For industrial deployment</li> <li>Industrial chassis form factor</li> <li>Reliability prioritized</li> </ul>"},{"location":"Papers/index.html","title":"Papers","text":""},{"location":"Papers/index.html#learn-about-reading-academic-papers","title":"Learn about reading academic papers","text":"<p>How to read a paper by S. Keshav</p>"},{"location":"Programming/index.html","title":"Programming","text":""},{"location":"Programming/index.html#language","title":"Language","text":"<ul> <li>Python</li> <li>C++</li> <li>SQL</li> </ul>"},{"location":"Programming/C%2B%2B/index.html","title":"C++","text":""},{"location":"Programming/Other_Languages/index.html","title":"Other languages","text":""},{"location":"Programming/Other_Languages/index.html#data","title":"Data","text":"<ol> <li>SQL</li> </ol>"},{"location":"Programming/Other_Languages/sql.html","title":"SQL","text":""},{"location":"Programming/Other_Languages/sql.html#data-type","title":"Data type","text":"<ol> <li>INTEGER</li> <li>REAL</li> <li>TEXT</li> <li>CHAR</li> <li>VARCHAR</li> <li>DATE</li> </ol>"},{"location":"Programming/Other_Languages/sql.html#constraints","title":"Constraints","text":"<ol> <li>PRIMARY KEY</li> <li>UNIQUE</li> <li>NOT NULL</li> <li>DEFAULT</li> <li>FOREIGN KEY, </li> <li>CHECK</li> <li>INDEX</li> </ol> <p>Clause is the command such as <code>CREATE</code>,<code>FROM</code></p>"},{"location":"Programming/Other_Languages/sql.html#manipulation","title":"Manipulation","text":"<pre><code>CREATE TABLE table_name (\u00a0\u00a0\u00a0\n    id INTEGER,\n    name TEXT,\n    age INTEGER\n)\n\nINSERT INTO table (id, name, age) \nVALUES (1, 'Justin Bieber', 29);\n\nSELECT name FROM celebs;\nSELECT * FROM celebs;\n\nALTER TABLE celebs\nADD COLUMN twitter_handle TEXT;\n\nUPDATE celebs\nSET twitter_hand le = \"@taylorswift13\"\nWHERE id = 4;\n\nDELETE FROM celebs\nWHERE twitter_handle IS NULL;\n\nAS\n\nLIKE\n\nBETWEEN\n\nOR\n\nORDER BY\n\nLIMIT\n\n\nCASE\n    WHEN xx THEN\n    ELSE xx\nEND\n\nCOUNT\n\nSUM\n\nMAX/MIN\n\nAVERAGE\n\nROUND\n\nGROUP BY\n\nHAVING\n</code></pre>"},{"location":"Programming/Python/index.html","title":"Python","text":""},{"location":"Programming/Python/index.html#built-in-libraries","title":"Built-in libraries","text":"<ul> <li>OS</li> <li>json</li> </ul>"},{"location":"Programming/Python/index.html#properties","title":"Properties","text":"<ul> <li> <p>lambda Use when function is simple, since its only one line. And make code clearer.</p> </li> <li> <p>CPython</p> </li> <li>Everything is an Object</li> <li>Automatic Memory Management<ul> <li>Reference Counting</li> <li>Generational Garbage Collection</li> </ul> </li> <li>C API</li> <li>GIL</li> </ul>"},{"location":"Programming/Python/json.html","title":"JSON","text":""},{"location":"Programming/Python/os_py.html","title":"OS","text":""},{"location":"Programming/Python/os_py.html#functions","title":"Functions:","text":"<ul> <li>os.path.join(): Join one or more path segments intelligently.</li> </ul>"},{"location":"Robotics/index.html","title":"Index","text":""},{"location":"Robotics/index.html#frameworks","title":"Frameworks","text":"<ul> <li>ROS2</li> </ul>"},{"location":"Robotics/ROS/index.html","title":"Basic Concepts","text":"<ul> <li>RQT</li> <li>ROS2 Node</li> <li>ROS2 topics</li> <li>ROS2 publisher</li> <li>ROS2 subscriber</li> <li>ROS2 service</li> <li>ROS2 actions</li> <li>ROS2 parameters</li> <li>Colcon</li> </ul>"},{"location":"Robotics/ROS/index.html#publisher-subscriber-topic-model","title":"Publisher--subscriber-topic model","text":"<ol> <li>1 publisher can send message to only 1 topic, since each topic receive only one message type</li> <li>1 node can have multiple publishers, and send to different topics</li> <li>1 topic can have multiple publishers and subscribers</li> <li>all message send to one topic should be the same message type</li> </ol>"},{"location":"Robotics/ROS/index.html#call-response-service-model","title":"Call-response-service model","text":"<ol> <li>1 server can support multiple services</li> <li>1 client can call multiple services</li> <li>1 service can only have 1 server</li> <li>1 service can have multiple clients</li> </ol>"},{"location":"Robotics/ROS/index.html#difference-between-topic-and-service","title":"Difference between topic and service","text":"<p>topics allow node to subscribe to data streams and get continual updates, services only provide data when they are specifically called by a client</p> Topic Service Sensor streaming One-time operation Status information Computation task Control command Config and setting log Execute an action sequence"},{"location":"Robotics/ROS/index.html#logger-levels","title":"Logger levels","text":"<ul> <li>Fatal: indicate the system is going to terminate to try to protect itself from detriment</li> <li>Error: indicate significant issues that won't necessarily damage the system, but are preventing it from functioning properly</li> <li>warn: indicate unexpected activity or non-ideal results that might represent a deeper issue, but don't harm functionality outright</li> <li>info: indicate event and status updates that serve as a visual verification that the system is running as expected</li> <li>debug: detail the entire step-by-step process of the system execution</li> <li>how to use <ul> <li>set to debug -&gt; display all logs</li> <li>set to info -&gt; display info, warn, error, fatal</li> <li>set to warn -&gt; display warn, error, fatal</li> <li>Developing stage: debug</li> <li>Testing stage: info</li> <li>Production stage: warn</li> </ul> </li> </ul>"},{"location":"Robotics/ROS/index.html#launching-nodes","title":"Launching nodes","text":"<p>allow you to start up and configure a number of executables containing ROS 2 node simultaneously. Running a single launch file with the <code>ros2 launch</code> command will start up your entire system - all nodes and their configurations - at once.</p>"},{"location":"Robotics/ROS/index.html#recording-and-playing-back-data","title":"Recording and playing back data","text":"<p>the data will be accumulated in a new bag directory with a name in the pattern of <code>rosbag2_year_month_day_hour_minute_second</code>. This dirctory will contain a <code>metadata.yaml</code></p> <pre><code># record data published to a topic\nros2 bag record &lt;topic_name&gt;\n\n# record multiple topics\nros2 bag record -o subset /turtle1/cmd_vel /turtle1/pose\n# -o options allows you to choos a unique name for your bag file\n# to record more than one topic at at ime, simply list each topic separated by a space\n\n# see details about your recording\nros2 bag info &lt;bag_file_name&gt;\n</code></pre> <p>Before replaying the bag file, enter ctrl+c in the terminal where the teleop is running. Then make sure your turtlesim window is visible so you can see the bag file is action.</p> <pre><code>ros2 bag play &lt;bagi_file_name&gt;\n</code></pre>"},{"location":"Robotics/ROS/index.html#workspace","title":"Workspace","text":"<ul> <li>Subdirectory<ul> <li>src: It is where the source code of ROS packages will be located</li> <li>build: create by colcon, where intermediate files are stored. For each package a subfolder will be created in which e.g. CMake is being invoked</li> <li>install: It is where each packages will be installed to. By default each package will be installed into a separate subdirectory</li> <li>log: It contains various logging information about each colcon invocation.</li> </ul> </li> <li>underlay: </li> <li>Create the workspace<ul> <li>before building the workspace, you need to resolve the package dependencies, from the root of your workspace <code>rosdep install -i --from-path srsc --rosdistro humble -y</code></li> <li>source underlay</li> <li>source overlay in the root     <code>source install/local_setup.zsh</code></li> </ul> </li> </ul>"},{"location":"Robotics/ROS/index.html#package","title":"Package","text":"<ul> <li>A package is an organizational unit for your ROS2 Code. Used to share with others, and you can release your ROS2 work and allow others to build and use it easily.</li> <li>Minimum required contents<ul> <li>CMake<ul> <li>CMakeLists.txt</li> <li>include/ <li>package.xn=ml</li> <li>src</li> <li>Python<ul> <li>package.xml</li> <li>resource/\\ <li>setup.cfg</li> <li>setup.py</li> <li> <pre><code>ros2 pkg create --build-type ament-python --license Apache-2.0 --node name  &lt;package name&gt; &lt;node name&gt; -- dependencies &lt;dependency-name&gt;\n</code></pre> <ul> <li>Content inside package.xml and setup.py shoule be the same.</li> </ul>"},{"location":"Robotics/ROS/index.html#ament","title":"Ament","text":"<p>It is a build system and packaging tool, designed to replace ROS1's catkin, think it as a specialized \"compiler and package manager\" for ROS 2. Ament toolset: 1. ament_cmake 2. ament_python 3. ...</p>"},{"location":"Robotics/ROS/RQT.html","title":"RQT","text":"<p>RQT is a graphical user interface (GUI) framework for ROS (Robot Operating System) that provides a centralized environment for running various tools and interfaces as plugins</p>"},{"location":"Robotics/ROS/RQT.html#basic-commands","title":"Basic commands","text":"<pre><code># run rqt\nrqt\n\n# start rqt_console\nros2 run rqt_console rqt_console\n\n</code></pre>"},{"location":"Robotics/ROS/RQT.html#rqt_console","title":"rqt_console","text":"<p>It is used to introspect log messages in ROS2. It has three sections - The first section of the console is where log messages from your system will display - In the middle you have the option to filter messages by excluding severity levels. You can also also add more exclusion filters using the plus-sign button to the right - The button section is for highlighting messages that include a string you input</p>"},{"location":"Robotics/ROS/colcon.html","title":"Colcon","text":""},{"location":"Robotics/ROS/colcon.html#in-ros2","title":"In ROS2","text":"<ul> <li>Before we build, we should source the underlay first, which is the ros humble setup.zsh file</li> <li>In the root of the workspace, run <code>colcon build</code>. </li> </ul> <pre><code>colcon build --symlink-install\ncolcon build --packages-up-to\ncolcon build --event-handlers console_direct+\n# --symlink-install allow the installed files to be changed by changing the files in the source space without build again.(e.g. python files or other non-compiled resources, except c++)\n# --packages-up-to: builds thie package you want, plus all its dependencies, but not the whole workspace(save time)\n# --event-handlers console_direct+ shows console output while building\n\n</code></pre> <ul> <li>run test for the packages just built, run </li> </ul> <pre><code>colcon test\n</code></pre> <ul> <li>when colcon has completed building successfully, the outpt will be in the install directory. We need to source all required elements to our path  before we we can use any of the installed executables or libraries. run </li> </ul> <pre><code>source install/setup.zsh\n</code></pre> <ul> <li>after all steps above, we can run some nodes</li> </ul> <pre><code>ros2 run xxx_subscriber subscriber_function\nros2 run xxx_publisher publisher_function\n</code></pre> <ul> <li>If you do not want to build a specific package place an empty file named COLCON_IGNORE in the directory</li> </ul>"},{"location":"Robotics/ROS/colcon.html#build-python-package","title":"build python package","text":"<ol> <li>write source code</li> <li>change setup.py: create command tools, install python library</li> <li>change package.xml: define ros2 dependency like rclpy, std_msgs, tell colcon build type</li> <li>setup.cfg: set installation path </li> </ol>"},{"location":"Robotics/ROS/ros2_actions.html","title":"Ros2 actions","text":"<p>Actions are one of the communication types in ROS2 and are intended for long running tasks. They consist of three parts: a goal, feedback and a result</p> <p>Actions are built on topics and services. Their functionality is similar to services, except actions are preemptable. They also provide steady feedback, as opposed to services which return a single response.</p> <p>Actions use a client-server model. An \"action client\" node sends a goal to an \"action server\" node that acknowledges the goal and returns a stream of feedback and a result</p>"},{"location":"Robotics/ROS/ros2_actions.html#very-similar-to-topic-service-continuous-data-waiting-for-response-can-be-cancel-feedback-for-progress","title":"Very similar to Topic + Service, continuous data, waiting for response, can be cancel, feedback for progress","text":""},{"location":"Robotics/ROS/ros2_actions.html#basic-commands","title":"Basic commands","text":"<pre><code># Identify all the actions in the ROS graph\nros2 action list &lt;-t&gt;\n# &lt;-t&gt; option shows all actions's action type\n\n# infomation about action detail, action clients, action servers\nros2 action info &lt;action_name&gt;\n\n# show structure of the action type\nros2 interface show &lt;action_type&gt;\n# The sction of this message above thei first --- is the structure(data type and name) of the goal request. The next section is the structure of teh result. The last section is the structure of the feedback.\n\n# send an action goal\nros2 action send_goal &lt;action_name&gt; &lt;action_type&gt; &lt;values&gt; &lt;--feedback&gt;\n# &lt;value &gt; in YAML format\n# &lt;--feedback&gt; will see th feedback of this goal\n</code></pre>"},{"location":"Robotics/ROS/ros2_node.html","title":"Ros2 node","text":"<ol> <li>Each node responsible for a single modular purpose</li> <li>Each node can send and receive data from other nodes via topics, service, actions, or parameters</li> <li>a single executable can contain one or more nodes.</li> </ol>"},{"location":"Robotics/ROS/ros2_node.html#basic-commands","title":"Basic Commands","text":"<pre><code># =================================================\n# Node\n# =================================================\n# launch an executable from a package\nros2 run &lt;package_name&gt; &lt;executable_name&gt;\n\n# list all nodes' name\nros2 node list\n\n# reassign default node properties, like node name, topic names\nros2 run turtlesim turtlesim_node --ros-args --remap __node:=my_turtle\n\n# access more information(publisher, subscriber, service, action) about node\nros2 node info &lt;node_name&gt;\n</code></pre>"},{"location":"Robotics/ROS/ros2_parameters.html","title":"Ros2 parameters","text":"<p>It is a configuration value of a node. You can think of parameters as node settings. A node can store parameters as integers, floats, boolean, strings and lists.</p>"},{"location":"Robotics/ROS/ros2_parameters.html#basic-commands","title":"Basic Commands","text":"<pre><code># =================================================\n# Parameters\n# =================================================\n# list parameters belonging to your nodes\nros2 param list\n\n# display the type and current value of a parameter\nros2 param get &lt;node_name&gt; &lt;parameter_name&gt;\n\n# change a parameter's value at runtime\nros2 param set &lt;node_name&gt; &lt;parameter_name&gt; &lt;value&gt;\n\n# save all of a node's current parameter values into a file\nros2 param dump &lt;node_name&gt;\n\n# load parameters from a file to a currently running node\nros2 param load &lt;node_name&gt; &lt;parameter_file&gt;\n\n# load parameter file on node startup\nros2 run &lt;package_name&gt; &lt;executable_name&gt; --ros-args --params-file &lt;file_name&gt;\n</code></pre>"},{"location":"Robotics/ROS/ros2_parameters.html#shared-parameter-for-all-node","title":"Shared parameter for all node","text":"<ol> <li>use_sim_time</li> </ol>"},{"location":"Robotics/ROS/ros2_publisher.html","title":"Ros2 publisher","text":""},{"location":"Robotics/ROS/ros2_publisher.html#createa-a-publisher","title":"Createa a publisher","text":"<ol> <li>Init<ol> <li>inherit from parent</li> <li>define messages of publisher node(data type, topic name, queue size)<ul> <li>queue size: It's the number of message that will help subscriber to hold  if subscribe received message slowly, rather than control the sending speed</li> </ul> </li> <li>set a timer, used to control sending speed</li> <li>set a counter self.i used in callback</li> </ol> </li> <li>set timer_callback<ol> <li>it creates a message with the counter value appended, and publishes it to the console with get_logger().info</li> </ol> </li> <li>set main function<ol> <li>rclpy.init(args=args)</li> <li>define node in a variable</li> <li>called rclpy.spin(node) to repeattedly called timer_callback until type ctrl+c the loop then close</li> <li>(optional) publisher.destroy_node()</li> <li>rclpy.shotdown()</li> </ol> </li> <li>Add dependency in setup.py, setup.cfg and package.xml</li> </ol>"},{"location":"Robotics/ROS/ros2_service.html","title":"Ros2 service","text":"<p>Services are another method of communication for nodes. Services are based on a call-and-response model. While topics allow nodes to subscribe to data streams and get continual updates, services only provide data when they are specifically called by a client.</p>"},{"location":"Robotics/ROS/ros2_service.html#basic-commands","title":"Basic Commands","text":"<pre><code># =================================================\n# Service\n# =================================================\n# list all the services currently active in the system\nros2 service list &lt;-t&gt;\n# -t option allow us to list all service that contain service type\n\n# find out the type of a service\nros2 service type &lt;serivce_name&gt;\n\n# if you want to find all the services of a specific type\nros service find &lt;type_name&gt;\n\n# can call services from the command line, but we need to know the structure of the input arguments\nros2 interface show &lt;type_name&gt;.srv\n# THe --- separates the request structure(above) from the response structure(below).\n\n# call a service \nros2 service call &lt;service_name&gt; &lt;service_type&gt; &lt;arguments&gt;\n</code></pre>"},{"location":"Robotics/ROS/ros2_service.html#create-a-service","title":"Create a service","text":"<ol> <li>init<ol> <li>inherit Node and name it</li> <li>create service(service type, name, callback)</li> </ol> </li> <li>define callback</li> </ol>"},{"location":"Robotics/ROS/ros2_service.html#create-a-client","title":"Create a client","text":"<ol> <li>init<ol> <li>inherit Node and name it</li> <li>the type and name must match for the client and service to be able to communicate</li> <li>while loop in the constructor checks if a service matching the type and name of the client is available once a second.</li> <li>Create a new request object</li> <li>define send request method</li> </ol> </li> <li>main<ol> <li>init rclpy</li> <li>construct object</li> <li>send request</li> <li>spin_until_future_complete</li> <li>log resultx</li> </ol> </li> </ol>"},{"location":"Robotics/ROS/ros2_service.html#create-custom-msg-and-srv-files","title":"Create custom msg and srv files","text":"<ol> <li>create a new package</li> <li>mkdir msg srv</li> <li>msg definition</li> <li>srv definition</li> <li>change Cmakelist.txt</li> <li>add dependency in package.xml</li> <li>build</li> </ol>"},{"location":"Robotics/ROS/ros2_subscriber.html","title":"Ros2 subscriber","text":""},{"location":"Robotics/ROS/ros2_subscriber.html#create-a-subscriber","title":"Create a subscriber","text":"<ol> <li>init<ol> <li>inherit from Node and name itself</li> <li>define message in subscription: data type, topic name, listener_callback function, queue size<ol> <li>data type and topic name should be the same as the publisher you want to subscribe</li> <li>listener_callback function doesn't need any timer, its callback get called as soon as it receives a message</li> </ol> </li> </ol> </li> <li>listener_callback<ol> <li>simply print an info message to the console</li> </ol> </li> <li>main<ol> <li>almost same as publisher, replacing the creation and spining of the publisher with the subscriber</li> </ol> </li> </ol>"},{"location":"Robotics/ROS/ros2_topics.html","title":"Ros2 topics","text":"<ol> <li>Act as a bus for node to exchange messages</li> <li>a node may publish data to any number of topics and simultaneously have subscriptions to any number of topics</li> </ol>"},{"location":"Robotics/ROS/ros2_topics.html#topic-type","title":"topic type","text":"<p>how nodes know they're talking about the same information as it moves over topics</p>"},{"location":"Robotics/ROS/ros2_topics.html#basic-commands","title":"Basic Commands","text":"<pre><code># =================================================\n# Topic\n# =================================================\n# list all topics currently active in the system\nros2 topic list &lt;-t&gt;\n# -t option allow us to list all topic that contain topic type\n\n# See the data being published on topic\nros2 topic echo &lt;topic_name&gt;\n\n# See topic information\nros2 tokpic info /turtle1/cmd_vel\n\n# IF you have the mesage structure, you can publish data onto a topic directly\nros2 topic pub &lt;--once/--rate 1&gt; &lt;topic_name&gt; &lt;msg_type&gt; '&lt;args&gt;'\n#&lt;--once&gt; option means publish one message then exit\n#&lt;--rate 1&gt; option means publish the command in a steady stream at 1hz\n\n# view the rate at which data is published using\nros2 topic hz &lt;topic_name&gt;\n\n</code></pre>"},{"location":"Robotics/ROS/ros2_topics.html#how-to-create-publisher-subscriber-stystem","title":"How to create publisher-subscriber stystem","text":"<p>publisher subscriber</p>"},{"location":"Systems/index.html","title":"Index","text":""},{"location":"Systems/index.html#development","title":"Development","text":"<ul> <li>Linux commands</li> <li>zsh</li> <li>ssh</li> <li>Jupyter</li> <li>Docker</li> <li>TMUX</li> </ul>"},{"location":"Systems/jupyter.html","title":"Jupyter","text":""},{"location":"Systems/jupyter.html#magic-commands","title":"Magic commands","text":"<ol> <li>%matplotlib inline: tells Jupyter to display Matplotlib plots and figures directly within the notebook's output cells, rather than opening them in separate window.</li> </ol>"},{"location":"Systems/linux_command.html","title":"Linux command","text":"<ol> <li>rg: recursively searches the current directory for a regrex pattern</li> </ol>"},{"location":"Systems/ssh.html","title":"Ssh","text":""},{"location":"Systems/ssh.html#tips","title":"Tips","text":"<ol> <li>Access remote services (like Jupyter Notebook or web servers) on your local machine through SSH tunneling.</li> </ol> <p><code>ssh -L [local_port]:localhost:[remote_port] username@remote_server</code></p>"},{"location":"Systems/tmux.html","title":"TMUX","text":"<p>Tmux allows user to manage multiple terminal sessions, split windows into panes, and run programs in the background even if you disconnect.</p> <p>To be more ergonomic, I change my prefix bind-key to ctrl+s and I use catppuccin/tmux to beaufiful it.</p> <p>Here is a cheatsheet for tmux: </p>"},{"location":"Systems/zsh.html","title":"Zsh","text":""},{"location":"Systems/zsh.html#bindkey","title":"Bindkey","text":"<ol> <li>Ctrl+R: search history commands</li> </ol>"},{"location":"Tools/index.html","title":"Tools","text":""},{"location":"Tools/index.html#documentation","title":"Documentation","text":"<ul> <li>LaTeX</li> </ul>"},{"location":"Tools/index.html#cloud-service","title":"Cloud service","text":"<ul> <li>AWS</li> </ul>"},{"location":"Tools/index.html#nvidia-software-ecosystem","title":"Nvidia Software Ecosystem","text":"<p>Home Page</p> <ul> <li>CUDA</li> <li>cuDNN</li> <li>TensorRT</li> <li>NCCL</li> <li>NGC</li> <li>Deep Learning Frameworks Support</li> <li>Nsight</li> <li>DLProf</li> <li>Nvidia AI Enterprise</li> </ul> <p>If you interested in Nvidia's Hardware Ecosystem: Nvidia Hardware Ecosystem</p>"},{"location":"Tools/aws.html","title":"AWS","text":""},{"location":"Tools/aws.html#computing","title":"Computing","text":"<p>EC2: Virtual server, basic computing service</p> <ul> <li>P series<ul> <li>P6: B100</li> <li>P5: H100</li> <li>P5en: H200</li> <li>P4: A100</li> <li>P3: V100</li> </ul> </li> <li>G series<ul> <li>G5: A10G</li> <li>G4dn: T4</li> </ul> </li> </ul>"},{"location":"Tools/aws.html#storage","title":"Storage","text":"<p>S3: Object storage, store file and data</p>"},{"location":"Tools/latex.html","title":"LaTeX","text":""},{"location":"Tools/Nvidia/index.html","title":"Nvidia Software Ecosystem","text":"<ul> <li>CUDA</li> <li>cuDNN</li> <li>TensorRT</li> <li>NCCL</li> <li>NGC</li> <li>Deep Learning Frameworks Support</li> <li>Nsight</li> <li>DLProf</li> <li>Nvidia AI Enterprise</li> </ul>"},{"location":"Tools/Nvidia/cuda.html","title":"CUDA","text":"<p>It is a parallel computing platform and programming model developed by NVIDIA that allows developers to use the processing power of NVIDIA GPUs for general-purpose computing, not just graphics.</p>"},{"location":"Tools/Nvidia/cudnn.html","title":"cuDNN","text":"<p>It is a GPU-accelerated library from NVIDIA for deep learning, which stand for Cuda Deep Neural Network library.</p>"},{"location":"Tools/Nvidia/dl_frameworks.html","title":"Deep Learning Frameworks Support","text":""},{"location":"Tools/Nvidia/dlprof.html","title":"DLProf","text":"<p>It is a tool designed to help data scientists analyze and improve the performance of their deep learning models.</p>"},{"location":"Tools/Nvidia/nccl.html","title":"NCCL","text":"<p>It is a library of routines for high-performance communication between multiple GPUs, used in deep learning and high-performance computing (HPC). </p> <p>Learn what is DDP</p>"},{"location":"Tools/Nvidia/ngc.html","title":"NGC","text":"<p>It is a portal of enterprise services, software, and support for AI, digital twins, and high-performance computing (HPC).</p>"},{"location":"Tools/Nvidia/nsight.html","title":"Nsight","text":"<p>It is a system-wide performance analysis tool designed to visualize an applivation's algorithms.</p>"},{"location":"Tools/Nvidia/nsight.html#features","title":"Features","text":"<ul> <li>Visualize CPU-GPU interactions</li> <li>Track GPU Activity</li> <li>Trace GPU Workloads</li> <li>Accelerate Multi-Node performace</li> <li>Optimize Python for AI and deep learning</li> <li>Detect Frame Stutter and Bottlenecks</li> </ul>"},{"location":"Tools/Nvidia/nvaie.html","title":"Nvidia AI Enterprise","text":"<p>It is a cloud-native suite of software tools, libraries, and frameworks, including NVIDIA NIM and NeMo microservices, that accelerate and simplify the development, deployment, and scaling of AI applications.</p>"},{"location":"Tools/Nvidia/nvaie.html#nvnim","title":"NVNIM","text":""},{"location":"Tools/Nvidia/nvaie.html#nvnemo","title":"NVNeMo","text":""},{"location":"Tools/Nvidia/tensorrt.html","title":"TensorRT","text":"<p>It is an NVIDIA SDK that provides a high-performance deep learning inference optimizer and runtime for deploying trained neural networks on NVIDIA GPUs.</p>"}]}